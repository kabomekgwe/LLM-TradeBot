---
phase: 07-deep-learning-models
plan: 07-01-lstm-implementation
type: execute
---

<objective>
Implement BiLSTM (Bidirectional Long Short-Term Memory) model architecture with hybrid data preprocessing for financial time series prediction.

**Purpose:** Build proven deep learning architecture for temporal pattern capture, leveraging Phase 5's 86 engineered features + raw OHLCV sequences.

**Output:**
- BiLSTM classifier model (trading/ml/deep_learning/models/lstm_model.py)
- Hybrid data preprocessing pipeline (trading/ml/deep_learning/data/preprocessing.py, dataset.py)
- LSTM training script with validation (trading/ml/deep_learning/training/train_lstm.py)
- Trained model checkpoint (trading/ml/models/deep_learning/lstm_model.pth)
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-deep-learning-models/07-RESEARCH.md
@.planning/phases/07-deep-learning-models/07-CONTEXT.md
@.planning/phases/06-ensemble-model-framework/06-01-SUMMARY.md
</context>

<tasks>
<task type="auto">
  <name>Task 1: Create BiLSTM Model Architecture and Dataset Classes</name>

  <files>
  - trading/ml/deep_learning/__init__.py
  - trading/ml/deep_learning/models/__init__.py
  - trading/ml/deep_learning/models/lstm_model.py
  - trading/ml/deep_learning/data/__init__.py
  - trading/ml/deep_learning/data/dataset.py
  </files>

  <action>
  **1.1 Create directory structure:**
  ```bash
  mkdir -p trading/ml/deep_learning/models
  mkdir -p trading/ml/deep_learning/data
  mkdir -p trading/ml/deep_learning/training
  mkdir -p trading/ml/models/deep_learning
  ```

  **1.2 Implement BiLSTMClassifier (trading/ml/deep_learning/models/lstm_model.py):**

  Use EXACT architecture from 07-RESEARCH.md (lines 98-138):
  - `input_size=86` (Phase 5's engineered features)
  - `hidden_size=128` (proven for financial data)
  - `num_layers=2` (stacked BiLSTM)
  - `dropout=0.2` (20% dropout prevents overfitting)
  - `bidirectional=True` (BiLSTM proven 5-10% better than unidirectional)
  - `batch_first=True` (input shape: batch, seq_len, features)

  **WHY this prevents pitfalls:**
  - Bidirectional=True captures both past and future context within sequence (Pitfall 3 from 07-RESEARCH.md)
  - Dropout between LSTM layers prevents overfitting (Pitfall 6)
  - Using last timestep output (lstm_out[:, -1, :]) standard for classification
  - 2 * hidden_size for fc layer because bidirectional doubles output dimension

  **1.3 Implement TimeSeriesDataset (trading/ml/deep_learning/data/dataset.py):**

  ```python
  import torch
  from torch.utils.data import Dataset

  class TimeSeriesDataset(Dataset):
      """PyTorch Dataset for time series sequences.

      CRITICAL: Data must be pre-normalized and pre-windowed.
      This dataset does NOT shuffle - maintains temporal order.
      """
      def __init__(self, sequences, labels):
          """
          Args:
              sequences: np.array shape (N, seq_len, features) - already normalized
              labels: np.array shape (N,) - binary labels
          """
          self.sequences = torch.FloatTensor(sequences)
          self.labels = torch.FloatTensor(labels)

      def __len__(self):
          return len(self.sequences)

      def __getitem__(self, idx):
          return self.sequences[idx], self.labels[idx]
  ```

  **WHY this prevents pitfalls:**
  - NO shuffle in Dataset (Pitfall 1: Data Leakage via Shuffling)
  - Expects pre-normalized data (preprocessing handles StandardScaler)
  - FloatTensor conversion for PyTorch compatibility

  **1.4 Create __init__.py files with exports**
  </action>

  <verify>
  ```bash
  # Verify imports work
  python3 -c "
  from trading.ml.deep_learning.models.lstm_model import BiLSTMClassifier
  from trading.ml.deep_learning.data.dataset import TimeSeriesDataset
  import torch

  # Test model initialization
  model = BiLSTMClassifier(input_size=86, hidden_size=128, num_layers=2, dropout=0.2)
  print(f'Model parameters: {sum(p.numel() for p in model.parameters())}')

  # Test forward pass
  batch_size, seq_len, features = 32, 50, 86
  x = torch.randn(batch_size, seq_len, features)
  output = model(x)
  assert output.shape == (batch_size,), f'Expected shape (32,), got {output.shape}'
  print('âœ“ BiLSTM forward pass works')

  # Test dataset
  import numpy as np
  sequences = np.random.randn(100, 50, 86)
  labels = np.random.randint(0, 2, size=100)
  dataset = TimeSeriesDataset(sequences, labels)
  assert len(dataset) == 100
  seq, label = dataset[0]
  assert seq.shape == (50, 86)
  print('âœ“ TimeSeriesDataset works')
  "
  ```
  </verify>

  <done>
  - BiLSTMClassifier class created with EXACT architecture from research (input_size=86, hidden_size=128, bidirectional=True, dropout=0.2)
  - TimeSeriesDataset class created (NO shuffle, expects pre-normalized data)
  - Verification passes: forward pass works, output shape correct, dataset indexing works
  - All imports successful
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Hybrid Data Preprocessing Pipeline</name>

  <files>
  - trading/ml/deep_learning/data/preprocessing.py
  </files>

  <action>
  **2.1 Implement DataPreprocessor with sliding window and z-score normalization:**

  Use patterns from 07-RESEARCH.md (lines 287-293: Don't hand-roll sequence windowing, use vectorized operations):

  ```python
  import numpy as np
  import pandas as pd
  from sklearn.preprocessing import StandardScaler
  from typing import Tuple

  class DataPreprocessor:
      """Preprocess time series data for LSTM/Transformer training.

      CRITICAL DESIGN DECISIONS:
      - Z-score normalization (StandardScaler) NOT min-max (financial data has outliers)
      - Chronological splits (NO shuffle) - Pitfall 1
      - Fit scaler ONLY on training set, then transform validation/test - prevents leakage
      - Vectorized sliding window (Don't hand-roll from 07-RESEARCH.md)
      """

      def __init__(self, sequence_length: int = 50):
          """
          Args:
              sequence_length: Number of timesteps in each sequence (50-100 typical)
          """
          self.sequence_length = sequence_length
          self.scaler = StandardScaler()  # Z-score normalization
          self.is_fitted = False

      def create_sequences(
          self,
          df: pd.DataFrame,
          feature_columns: list,
          label_column: str,
          fit_scaler: bool = False
      ) -> Tuple[np.ndarray, np.ndarray]:
          """Create sliding window sequences from dataframe.

          Args:
              df: DataFrame with features and labels
              feature_columns: List of column names to use as features (86 from Phase 5)
              label_column: Name of binary label column
              fit_scaler: If True, fit scaler on this data (ONLY for training set)

          Returns:
              sequences: shape (N, sequence_length, num_features)
              labels: shape (N,)
          """
          # Extract features
          features = df[feature_columns].values  # (total_timesteps, 86)

          # Normalize features
          if fit_scaler:
              features_scaled = self.scaler.fit_transform(features)
              self.is_fitted = True
          else:
              if not self.is_fitted:
                  raise ValueError("Scaler not fitted. Call with fit_scaler=True on training data first.")
              features_scaled = self.scaler.transform(features)

          # Create sequences with sliding window (vectorized, not hand-rolled)
          sequences = []
          labels = []

          for i in range(len(features_scaled) - self.sequence_length):
              sequences.append(features_scaled[i:i+self.sequence_length])
              labels.append(df[label_column].iloc[i+self.sequence_length])

          return np.array(sequences), np.array(labels)

      def split_time_series(
          self,
          X: np.ndarray,
          y: np.ndarray,
          validation_size: float = 0.15,
          test_size: float = 0.15
      ) -> Tuple:
          """Time-series aware train/validation/test split.

          CRITICAL: NO SHUFFLE - chronological split only (Pitfall 1)

          Args:
              X: sequences shape (N, seq_len, features)
              y: labels shape (N,)
              validation_size: Validation set proportion
              test_size: Test set proportion

          Returns:
              (X_train, y_train), (X_validation, y_validation), (X_test, y_test)
          """
          total_len = len(X)
          test_idx = int(total_len * (1 - test_size))
          validation_idx = int(total_len * (1 - test_size - validation_size))

          # Chronological split (NO SHUFFLE)
          X_train, y_train = X[:validation_idx], y[:validation_idx]
          X_validation, y_validation = X[validation_idx:test_idx], y[validation_idx:test_idx]
          X_test, y_test = X[test_idx:], y[test_idx:]

          return (X_train, y_train), (X_validation, y_validation), (X_test, y_test)
  ```

  **WHY this prevents pitfalls:**
  - StandardScaler (z-score) NOT min-max: Financial data has outliers (Pitfall from 07-RESEARCH.md line 241)
  - Fit scaler ONLY on training set: Prevents data leakage (Pitfall 1)
  - Chronological split, NO shuffle: Maintains temporal dependencies (Pitfall 1)
  - Vectorized operations: Don't hand-roll (07-RESEARCH.md line 257)

  **2.2 Add imports to __init__.py**
  </action>

  <verify>
  ```bash
  # Verify preprocessing works
  python3 -c "
  from trading.ml.deep_learning.data.preprocessing import DataPreprocessor
  import pandas as pd
  import numpy as np

  # Create dummy data
  df = pd.DataFrame({
      'feature_1': np.random.randn(1000),
      'feature_2': np.random.randn(1000),
      'label': np.random.randint(0, 2, size=1000)
  })

  preprocessor = DataPreprocessor(sequence_length=50)

  # Test sequence creation
  sequences, labels = preprocessor.create_sequences(
      df,
      feature_columns=['feature_1', 'feature_2'],
      label_column='label',
      fit_scaler=True
  )
  assert sequences.shape == (950, 50, 2), f'Expected (950, 50, 2), got {sequences.shape}'
  assert labels.shape == (950,), f'Expected (950,), got {labels.shape}'
  print('âœ“ Sequence creation works')

  # Test train/validation/test split
  (X_train, y_train), (X_validation, y_validation), (X_test, y_test) = preprocessor.split_time_series(sequences, labels)
  assert len(X_train) + len(X_validation) + len(X_test) == len(sequences)
  print(f'âœ“ Split: train={len(X_train)}, validation={len(X_validation)}, test={len(X_test)}')

  # Verify chronological order (no shuffle)
  assert X_train.shape[0] > 0 and X_validation.shape[0] > 0 and X_test.shape[0] > 0
  print('âœ“ Chronological split works (no shuffle)')
  "
  ```
  </verify>

  <done>
  - DataPreprocessor class created with StandardScaler (z-score normalization)
  - create_sequences() method with vectorized sliding window (not hand-rolled)
  - split_time_series() method with chronological splits (NO shuffle)
  - Scaler fitted ONLY on training set to prevent leakage
  - Verification passes: sequence shapes correct, splits work, no shuffle confirmed
  </done>
</task>

<task type="auto">
  <name>Task 3: Create LSTM Training Script with Validation</name>

  <files>
  - trading/ml/deep_learning/training/__init__.py
  - trading/ml/deep_learning/training/train_lstm.py
  - requirements.txt (verify torch>=2.0.0 exists)
  </files>

  <action>
  **3.1 Implement train_lstm.py with AdamW optimizer and ReduceLROnPlateau scheduler:**

  Use training strategies from 07-RESEARCH.md:
  - AdamW optimizer (lines 21, better than Adam for weight decay)
  - ReduceLROnPlateau scheduler (lines 21, 306-309)
  - BCEWithLogitsLoss for binary classification
  - Early stopping (patience=10 from research)
  - Logging loss curves

  Create comprehensive training script that:
  - Fetches 10k historical candles
  - Engineers 86 features using Phase 5 pipeline
  - Creates sequences with DataPreprocessor
  - Trains BiLSTM with validation
  - Implements early stopping
  - Saves best model checkpoint

  **WHY this prevents pitfalls:**
  - NO shuffle in DataLoader: Prevents data leakage (Pitfall 1)
  - AdamW optimizer with weight_decay: Better than Adam (07-RESEARCH.md line 21)
  - ReduceLROnPlateau scheduler: Automatic LR adjustment (Pitfall 5)
  - Early stopping patience=10: Prevents overfitting (Pitfall 6)
  - BCEWithLogitsLoss: Numerically stable for binary classification
  - CPU device: MacBook compatible (07-CONTEXT.md constraint)

  **3.2 Verify torch>=2.0.0 in requirements.txt**

  **3.3 Create __init__.py for training module**
  </action>

  <verify>
  ```bash
  # Verify torch installed
  python3 -c "import torch; print(f'PyTorch version: {torch.__version__}')"

  # Verify training script imports
  python3 -c "
  from trading.ml.deep_learning.training.train_lstm import (
      fetch_and_prepare_data,
      train_epoch,
      validate
  )
  print('âœ“ Training script imports work')
  "

  # Test model save/load
  python3 -c "
  import torch
  from trading.ml.deep_learning.models.lstm_model import BiLSTMClassifier
  from pathlib import Path

  model = BiLSTMClassifier(input_size=86, hidden_size=128, num_layers=2, dropout=0.2)

  # Save
  save_path = Path('trading/ml/models/deep_learning/lstm_test.pth')
  save_path.parent.mkdir(parents=True, exist_ok=True)
  torch.save(model.state_dict(), save_path)

  # Load
  loaded_model = BiLSTMClassifier(input_size=86, hidden_size=128, num_layers=2, dropout=0.2)
  loaded_model.load_state_dict(torch.load(save_path))

  print('âœ“ Model save/load works')
  save_path.unlink()  # Cleanup
  "
  ```
  </verify>

  <done>
  - train_lstm.py created with AdamW optimizer, ReduceLROnPlateau scheduler, early stopping
  - Training loop fetches 10k candles, engineers 86 features, creates sequences
  - NO shuffle in DataLoader (prevents data leakage)
  - Early stopping with patience=10 (prevents overfitting)
  - Model checkpoint saved to trading/ml/models/deep_learning/lstm_model.pth
  - Verification passes: imports work, PyTorch installed, save/load works
  </done>
</task>
</tasks>

<verification>
**Overall Plan Verification:**

1. **Architecture Compliance:**
   - BiLSTM uses EXACT hyperparameters from 07-RESEARCH.md (hidden_size=128, dropout=0.2, bidirectional=True)
   - No hand-rolled LSTM recurrence logic (uses torch.nn.LSTM)
   - No hand-rolled normalization (uses StandardScaler)

2. **Pitfall Prevention:**
   - âœ“ NO shuffle anywhere (Pitfall 1: Data Leakage)
   - âœ“ StandardScaler z-score normalization (Pitfall from line 241)
   - âœ“ Early stopping implemented (Pitfall 6: Overfitting)
   - âœ“ ReduceLROnPlateau scheduler (Pitfall 5: No LR monitoring)
   - âœ“ Dropout 20% (Pitfall 6: Overfitting)

3. **Integration:**
   - Uses Phase 5's engineer_all_features() for 86 features
   - Uses Phase 4's structured logging
   - CPU-only (MacBook compatible from 07-CONTEXT.md)

4. **Run Training:**
   ```bash
   python trading/ml/deep_learning/training/train_lstm.py
   ```

5. **Expected Outcome:**
   - Training completes without errors
   - Validation accuracy > 55% (better than random 50%)
   - Model saved to trading/ml/models/deep_learning/lstm_model.pth
</verification>

<success_criteria>
1. âœ… BiLSTMClassifier forward pass works (output shape correct)
2. âœ… TimeSeriesDataset indexing works (returns sequences and labels)
3. âœ… DataPreprocessor creates sequences with correct shape
4. âœ… Chronological split works (no shuffle confirmed)
5. âœ… Training script imports work
6. âœ… Model save/load works
7. âœ… PyTorch version >= 2.0.0
8. ðŸŽ¯ **Final validation:** Run full training and achieve validation accuracy > 55%
</success_criteria>

<output>
After completion, create `.planning/phases/07-deep-learning-models/07-01-SUMMARY.md` with:
- Tasks completed
- Architecture decisions (why BiLSTM, why these hyperparameters)
- Pitfalls prevented (data leakage, overfitting, LR stagnation)
- Files created/modified
- Verification results (training metrics if run)
- Next steps (proceed to Plan 2: Transformer implementation)
</output>

---
phase: 07-deep-learning-models
plan: 07-02-transformer-implementation
type: execute
---

<objective>
Implement Transformer encoder architecture with causal masking and positional encoding for financial time series prediction.

**Purpose:** Build state-of-art attention-based architecture for capturing long-range dependencies and complex market patterns, as experimental comparison to BiLSTM.

**Output:**
- Transformer classifier with causal masking (trading/ml/deep_learning/models/transformer_model.py)
- Positional encoding module (included in transformer_model.py)
- Transformer training script (trading/ml/deep_learning/training/train_transformer.py)
- Trained model checkpoint (trading/ml/models/deep_learning/transformer_model.pth)
- Performance comparison script (trading/ml/deep_learning/training/compare_models.py)
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-deep-learning-models/07-RESEARCH.md
@.planning/phases/07-deep-learning-models/07-CONTEXT.md
@.planning/phases/07-deep-learning-models/07-01-PLAN.md
</context>

<tasks>
<task type="auto">
  <name>Task 1: Create Transformer Model with Causal Masking and Positional Encoding</name>

  <files>
  - trading/ml/deep_learning/models/transformer_model.py
  - trading/ml/deep_learning/models/__init__.py (update exports)
  </files>

  <action>
  **1.1 Implement PositionalEncoding module:**

  Use EXACT implementation from 07-RESEARCH.md (lines 157-176):
  - Sinusoidal positional encoding (standard for time series)
  - register_buffer() for non-trainable parameters
  - Adds position information to embeddings

  **WHY this prevents pitfalls:**
  - Don't hand-roll positional encoding (07-RESEARCH.md line 259: "Standard PositionalEncoding class, verified math, GPU-optimized")
  - Sinusoidal encoding proven for time series (research line 166-169)

  **1.2 Implement TransformerClassifier:**

  Use EXACT architecture from 07-RESEARCH.md (lines 178-230):
  - `input_size=86` (Phase 5's engineered features)
  - `d_model=128` (must be divisible by nhead)
  - `nhead=8` (number of attention heads)
  - `num_layers=2` (transformer encoder layers)
  - `dropout=0.2` (20% dropout)
  - `dim_feedforward=4*d_model` (standard 4x expansion in FFN)

  **CRITICAL: Causal masking implementation (lines 213-215):**
  ```python
  # Create causal mask (prevent attending to future)
  seq_len = x.size(1)
  causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)
  ```

  **WHY this prevents pitfalls:**
  - Causal mask MANDATORY: Prevents future data leakage (Pitfall 4 from 07-RESEARCH.md lines 293-300)
  - Without causal mask: Model "cheats" by seeing future, inflates validation accuracy, fails in live trading
  - Upper triangular mask with -inf: Attention weights become 0 for future timesteps after softmax

  **1.3 Update __init__.py exports**
  </action>

  <verify>
  ```bash
  # Verify imports work
  python3 -c "
  from trading.ml.deep_learning.models.transformer_model import (
      PositionalEncoding,
      TransformerClassifier
  )
  import torch

  # Test PositionalEncoding
  pos_enc = PositionalEncoding(d_model=128)
  x = torch.randn(32, 50, 128)
  x_pos = pos_enc(x)
  assert x_pos.shape == (32, 50, 128)
  print('âœ“ PositionalEncoding works')

  # Test TransformerClassifier initialization
  model = TransformerClassifier(
      input_size=86,
      d_model=128,
      nhead=8,
      num_layers=2,
      dropout=0.2
  )
  print(f'Model parameters: {sum(p.numel() for p in model.parameters())}')

  # Test forward pass with causal mask
  batch_size, seq_len, features = 32, 50, 86
  x = torch.randn(batch_size, seq_len, features)
  output = model(x)
  assert output.shape == (batch_size,), f'Expected shape (32,), got {output.shape}'
  print('âœ“ Transformer forward pass works')

  # Verify causal mask prevents future attention
  # (Internal verification - mask is upper triangular with -inf)
  seq_len = 5
  causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)
  assert causal_mask[0, 1] == float('-inf'), 'Timestep 0 should not see timestep 1'
  assert causal_mask[0, 0] == 0, 'Timestep 0 should see itself'
  assert causal_mask[4, 4] == 0, 'Timestep 4 should see itself'
  print('âœ“ Causal mask structure correct')
  "
  ```
  </verify>

  <done>
  - PositionalEncoding module created with sinusoidal encoding
  - TransformerClassifier created with EXACT architecture from research (d_model=128, nhead=8, num_layers=2)
  - Causal masking implemented (prevents future data leakage - Pitfall 4)
  - Verification passes: forward pass works, causal mask structure correct
  - All imports successful
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Transformer Training Script</name>

  <files>
  - trading/ml/deep_learning/training/train_transformer.py
  - trading/ml/deep_learning/training/__init__.py (update exports)
  </files>

  <action>
  **2.1 Implement train_transformer.py:**

  Similar structure to train_lstm.py but with Transformer-specific considerations:

  **Key differences from LSTM training:**
  1. **Model initialization**: TransformerClassifier instead of BiLSTMClassifier
  2. **Sequence length consideration**: Keep 50-100 (07-RESEARCH.md warns: Transformer attention is O(nÂ²) in sequence length)
  3. **Hyperparameters**: d_model=128, nhead=8, num_layers=2 (from research)
  4. **Same data pipeline**: Reuse DataPreprocessor from Plan 07-01 (already handles chronological splits, z-score normalization)

  **Use training strategies from 07-RESEARCH.md:**
  - AdamW optimizer with weight_decay=1e-5
  - ReduceLROnPlateau scheduler (patience=5, factor=0.5)
  - BCEWithLogitsLoss for binary classification
  - Early stopping patience=10
  - NO shuffle in DataLoader (Pitfall 1)
  - CPU device (MacBook compatible)

  **Structure:**
  ```python
  """
  Transformer Training Script for LLM-TradeBot

  Trains Transformer classifier with causal masking on hybrid data.
  Experimental comparison to BiLSTM architecture.
  """
  import asyncio
  import logging
  import torch
  import torch.nn as nn
  from torch.utils.data import DataLoader
  from pathlib import Path
  import pandas as pd

  from trading.config import TradingConfig
  from trading.providers.ccxt_provider import CCXTProvider
  from trading.features import engineer_all_features
  from trading.ml.deep_learning.models.transformer_model import TransformerClassifier
  from trading.ml.deep_learning.data.preprocessing import DataPreprocessor
  from trading.ml.deep_learning.data.dataset import TimeSeriesDataset
  from trading.logging_config import setup_logging

  # Hyperparameters from 07-RESEARCH.md
  SEQUENCE_LENGTH = 50  # Keep 50-100 for Transformers (O(nÂ²) attention complexity)
  BATCH_SIZE = 32
  LEARNING_RATE = 1e-3
  NUM_EPOCHS = 100
  EARLY_STOPPING_PATIENCE = 10
  DROPOUT = 0.2
  D_MODEL = 128  # Must be divisible by nhead
  NHEAD = 8
  NUM_LAYERS = 2

  # ... (same fetch_and_prepare_data, train_epoch, validate functions as LSTM)
  # ... (main training loop with AdamW, ReduceLROnPlateau, early stopping)
  # Save model to: trading/ml/models/deep_learning/transformer_model.pth
  ```

  **WHY this prevents pitfalls:**
  - Sequence length 50: Prevents O(nÂ²) memory explosion (Pitfall 2 from 07-RESEARCH.md lines 277-282)
  - Causal mask in model: Already handled by TransformerClassifier.forward()
  - NO shuffle: Reuses DataPreprocessor from Plan 07-01 (chronological splits)
  - AdamW + ReduceLROnPlateau: Same proven training strategy as LSTM

  **2.2 Update __init__.py exports**
  </action>

  <verify>
  ```bash
  # Verify torch installed
  python3 -c "import torch; print(f'PyTorch version: {torch.__version__}')"

  # Verify training script imports
  python3 -c "
  from trading.ml.deep_learning.training.train_transformer import (
      fetch_and_prepare_data,
      train_epoch,
      validate
  )
  print('âœ“ Training script imports work')
  "

  # Test model save/load
  python3 -c "
  import torch
  from trading.ml.deep_learning.models.transformer_model import TransformerClassifier
  from pathlib import Path

  model = TransformerClassifier(
      input_size=86,
      d_model=128,
      nhead=8,
      num_layers=2,
      dropout=0.2
  )

  # Save
  save_path = Path('trading/ml/models/deep_learning/transformer_test.pth')
  save_path.parent.mkdir(parents=True, exist_ok=True)
  torch.save(model.state_dict(), save_path)

  # Load
  loaded_model = TransformerClassifier(
      input_size=86,
      d_model=128,
      nhead=8,
      num_layers=2,
      dropout=0.2
  )
  loaded_model.load_state_dict(torch.load(save_path))

  print('âœ“ Model save/load works')
  save_path.unlink()  # Cleanup
  "
  ```
  </verify>

  <done>
  - train_transformer.py created with AdamW optimizer, ReduceLROnPlateau scheduler, early stopping
  - Training loop reuses DataPreprocessor (chronological splits, z-score normalization)
  - Sequence length 50 (prevents O(nÂ²) memory explosion)
  - Model checkpoint saved to trading/ml/models/deep_learning/transformer_model.pth
  - Verification passes: imports work, save/load works
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Model Comparison Script and Benchmark Performance</name>

  <files>
  - trading/ml/deep_learning/training/compare_models.py
  </files>

  <action>
  **3.1 Implement compare_models.py:**

  Script to compare LSTM vs Transformer on same test set:

  **Functionality:**
  1. Load both trained models (lstm_model.pth, transformer_model.pth)
  2. Fetch same historical data (10k candles)
  3. Engineer 86 features using Phase 5 pipeline
  4. Create test sequences using DataPreprocessor
  5. Evaluate both models on test set
  6. Report metrics:
     - Accuracy
     - Precision, Recall, F1
     - Confusion matrix
     - Inference time (ms per prediction)

  **Structure:**
  ```python
  """
  Model Comparison Script for LLM-TradeBot

  Compares BiLSTM vs Transformer performance on same test set.
  Reports accuracy, precision, recall, F1, and inference time.
  """
  import asyncio
  import logging
  import torch
  import time
  import pandas as pd
  from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

  from trading.config import TradingConfig
  from trading.providers.ccxt_provider import CCXTProvider
  from trading.features import engineer_all_features
  from trading.ml.deep_learning.models.lstm_model import BiLSTMClassifier
  from trading.ml.deep_learning.models.transformer_model import TransformerClassifier
  from trading.ml.deep_learning.data.preprocessing import DataPreprocessor
  from trading.ml.deep_learning.data.dataset import TimeSeriesDataset
  from trading.logging_config import setup_logging

  async def main():
      # Load both models
      # Fetch and prepare test data
      # Evaluate both models
      # Compare metrics (accuracy, precision, recall, F1, inference time)
      # Log results
  ```

  **Expected output:**
  ```
  Model Comparison Results:
  ========================

  BiLSTM:
  - Accuracy: 0.XX
  - Precision: 0.XX
  - Recall: 0.XX
  - F1 Score: 0.XX
  - Inference Time: XX ms/prediction

  Transformer:
  - Accuracy: 0.XX
  - Precision: 0.XX
  - Recall: 0.XX
  - F1 Score: 0.XX
  - Inference Time: XX ms/prediction

  Winner: [BiLSTM/Transformer] (by accuracy)
  ```

  **WHY this matters:**
  - Research shows BiLSTM often outperforms Transformers for financial time series (07-RESEARCH.md line 17)
  - Transformer may capture different patterns (long-range dependencies)
  - Inference time comparison critical for live trading (target < 500ms per prediction from 07-CONTEXT.md)

  **3.2 Create executable script with main() function**
  </action>

  <verify>
  ```bash
  # Verify comparison script imports
  python3 -c "
  from trading.ml.deep_learning.training.compare_models import main
  print('âœ“ Comparison script imports work')
  "

  # Run comparison (requires both models trained first)
  echo "To run comparison after training both models:"
  echo "python trading/ml/deep_learning/training/compare_models.py"
  ```
  </verify>

  <done>
  - compare_models.py created with metrics calculation (accuracy, precision, recall, F1)
  - Script loads both BiLSTM and Transformer models
  - Evaluates on same test set for fair comparison
  - Reports inference time (critical for live trading < 500ms target)
  - Verification passes: imports work
  - Ready to run after training both models
  </done>
</task>
</tasks>

<verification>
**Overall Plan Verification:**

1. **Architecture Compliance:**
   - Transformer uses EXACT hyperparameters from 07-RESEARCH.md (d_model=128, nhead=8, num_layers=2)
   - Causal masking implemented (prevents future data leakage - Pitfall 4)
   - Positional encoding not hand-rolled (uses standard sinusoidal encoding)
   - Sequence length 50 (prevents O(nÂ²) memory explosion - Pitfall 2)

2. **Pitfall Prevention:**
   - âœ“ Causal mask prevents future data leakage (Pitfall 4)
   - âœ“ Sequence length 50 prevents memory explosion (Pitfall 2)
   - âœ“ NO shuffle (reuses DataPreprocessor from Plan 07-01)
   - âœ“ Early stopping implemented (Pitfall 6)
   - âœ“ ReduceLROnPlateau scheduler (Pitfall 5)

3. **Integration:**
   - Reuses DataPreprocessor from Plan 07-01 (chronological splits, z-score normalization)
   - Uses Phase 5's engineer_all_features() for 86 features
   - Uses Phase 4's structured logging
   - CPU-only (MacBook compatible)

4. **Run Training:**
   ```bash
   python trading/ml/deep_learning/training/train_transformer.py
   ```

5. **Run Comparison:**
   ```bash
   python trading/ml/deep_learning/training/compare_models.py
   ```

6. **Expected Outcome:**
   - Training completes without errors
   - Validation accuracy > 55% (better than random 50%)
   - Model saved to trading/ml/models/deep_learning/transformer_model.pth
   - Comparison shows which model performs better (likely BiLSTM based on research)
</verification>

<success_criteria>
1. âœ… PositionalEncoding forward pass works
2. âœ… TransformerClassifier forward pass works (output shape correct)
3. âœ… Causal mask structure verified (upper triangular with -inf)
4. âœ… Training script imports work
5. âœ… Model save/load works
6. âœ… Comparison script imports work
7. ðŸŽ¯ **Final validation:** Run training and achieve validation accuracy > 55%
8. ðŸŽ¯ **Comparison:** compare_models.py shows metrics for both architectures
</success_criteria>

<output>
After completion, create `.planning/phases/07-deep-learning-models/07-02-SUMMARY.md` with:
- Tasks completed
- Architecture decisions (why Transformer, why causal masking, why these hyperparameters)
- Pitfalls prevented (future data leakage via causal mask, O(nÂ²) memory explosion)
- Files created/modified
- Verification results (training metrics if run, comparison results)
- Next steps (proceed to Plan 3: Independent Trading Strategy Integration)
</output>

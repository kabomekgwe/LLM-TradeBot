---
phase: 08-model-evaluation-backtesting
plan: 08-02
type: execute
---

<objective>
Integrate BiLSTM and Transformer models with backtesting.py framework, implement realistic trading simulation with transaction costs, and generate comprehensive performance reports.

**Purpose:** Create production-ready backtesting infrastructure that evaluates deep learning models under realistic trading conditions and generates actionable performance insights.

**Output:**
- backtesting.py Strategy wrapper for deep learning models
- Realistic cost modeling (commission, slippage)
- Comprehensive performance reports with visualizations
- Model comparison framework (BiLSTM vs Transformer vs Ensemble vs Baseline)
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-model-evaluation-backtesting/08-RESEARCH.md
@.planning/phases/08-model-evaluation-backtesting/08-01-PLAN.md

**Dependencies:**
- Plan 08-01 MUST be completed first (walk-forward validation and metrics modules required)

**Prior decisions affecting this phase:**
- Phase 7: BiLSTM (616K params) and Transformer (408K params) models trained
- Phase 7: ModelPersistence loads models securely (state_dict only)
- Phase 5: FeatureEngineer produces 86 features (must reuse for consistent backtesting)
- Phase 4: Structured logging and custom exceptions required
- Phase 1: No unsafe serialization

**Research findings (from 08-RESEARCH.md):**
- Use backtesting.py for simplicity (vectorbt PRO is paid, Zipline deprecated)
- backtesting.py 0.6.5 released July 2025 (latest stable)
- Commission: 0.001 (0.1% per trade, realistic for crypto)
- Slippage: 10-20 basis points (0.001-0.002)
- Trade-on-close: False for realistic execution
- CRITICAL: Precompute predictions to avoid look-ahead bias

**Integration with Plan 08-01:**
- Plan 08-01 provides: WalkForwardValidator, comprehensive_metrics()
- This plan uses walk-forward validation for model evaluation
- Financial metrics from Plan 08-01 used in performance reports
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement backtesting.py Strategy Wrapper</name>

  <files>
  trading/ml/deep_learning/backtesting/__init__.py
  trading/ml/deep_learning/backtesting/strategy.py
  requirements.txt
  </files>

  <action>
  Create backtesting.py Strategy wrapper for deep learning models:

  **1. Install backtesting.py library:**
  - Add `backtesting>=0.3.3` to requirements.txt
  - Run `pip install backtesting`

  **2. Create DeepLearningStrategy wrapper in `trading/ml/deep_learning/backtesting/strategy.py`:**

  The strategy class wraps BiLSTM and Transformer models for backtesting.py framework.
  
  CRITICAL: Predictions must be precomputed BEFORE backtesting to avoid look-ahead bias.
  The strategy uses precomputed predictions (Series with DatetimeIndex) passed as parameter.
  
  Implementation pattern:
  - init() method: Verify predictions provided and align with backtest data
  - next() method: Execute trading logic using precomputed signal (NO model inference)
  - Long signal when prediction >= threshold and no position
  - Exit when prediction < threshold and position exists

  **3. Implement prediction precomputation helper:**
  
  Function to precompute model predictions for backtesting.
  Uses DataPreprocessor to create sequences and runs model inference.
  Returns Series with predictions aligned to DatetimeIndex.

  **4. Add structured logging:**
  - Use Phase 4's logging_config
  - Log backtest initialization
  - Include DecisionContext for correlation

  **5. Update __init__.py exports**

  **WHY this pattern:**
  - Precomputed predictions prevent look-ahead bias (08-RESEARCH Pitfall 2)
  - backtesting.py Strategy interface allows vectorized backtesting
  - Separation of model inference and backtesting (clean architecture)
  </action>

  <verify>
  ```bash
  # Install backtesting
  pip install backtesting

  # Syntax check
  python3 -c "from trading.ml.deep_learning.backtesting.strategy import DeepLearningStrategy, precompute_predictions; print('✓ Imports work')"

  # Quick integration test (no full model training)
  python3 -c "
  from trading.ml.deep_learning.backtesting.strategy import DeepLearningStrategy, precompute_predictions
  from backtesting import Backtest
  import pandas as pd
  import numpy as np

  # Create dummy OHLC data
  dates = pd.date_range('2023-01-01', periods=1000, freq='1H')
  data = pd.DataFrame({
      'Open': np.random.rand(1000) * 100 + 50000,
      'High': np.random.rand(1000) * 100 + 50100,
      'Low': np.random.rand(1000) * 100 + 49900,
      'Close': np.random.rand(1000) * 100 + 50000,
      'Volume': np.random.rand(1000) * 1000000
  }, index=dates)

  # Create dummy predictions
  predictions = pd.Series(np.random.rand(1000), index=dates)

  # Test backtest initialization
  bt = Backtest(
      data,
      DeepLearningStrategy,
      cash=10000,
      commission=0.001
  )

  print('✓ Backtesting.py Strategy wrapper works')
  "
  ```
  </verify>

  <done>
  - DeepLearningStrategy class created with backtesting.py interface
  - Precomputed predictions pattern implemented (prevents look-ahead bias)
  - precompute_predictions() helper function created
  - Structured logging integrated
  - Exports added to __init__.py
  - backtesting library installed
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Realistic Cost Modeling and Backtest Runner</name>

  <files>
  trading/ml/deep_learning/backtesting/runner.py
  trading/ml/deep_learning/backtesting/config.py
  </files>

  <action>
  Create backtest runner with realistic trading costs:

  **1. Create configuration in `trading/ml/deep_learning/backtesting/config.py`:**

  Backtesting configuration with realistic trading costs based on 08-RESEARCH.md:
  - Commission: 0.001 (0.1% per trade, realistic for crypto exchanges)
  - Slippage: 10-20 basis points (0.001-0.002)
  - Trade-on-close: False (realistic execution, no perfect fills)
  - Initial cash: 10000.0
  - Max position size: 0.95 (95% of capital)
  - Model parameters: sequence_length=60, prediction_threshold=0.5
  - Walk-forward validation: initial_train_size=252, test_size=60, step_size=30

  **2. Implement backtest runner in `trading/ml/deep_learning/backtesting/runner.py`:**

  BacktestRunner class with two methods:
  
  a) run_single_backtest(model, data, predictions):
     - Creates Backtest with realistic costs from config
     - Runs backtest with precomputed predictions
     - Calculates financial metrics using comprehensive_metrics() from Plan 08-01
     - Returns dict with backtest_stats, financial_metrics, trades
  
  b) run_walk_forward_backtest(model, X, y, data):
     - Uses WalkForwardValidator from Plan 08-01
     - Runs backtest for each fold
     - Precomputes predictions for each test period
     - Returns list of backtest results per fold

  **3. Add slippage modeling:**
  - backtesting.py doesn't have built-in slippage
  - Document in config that slippage is 15 bps
  - Note: Commission of 0.001 includes slippage estimate

  **4. Add structured logging for backtest execution**

  **WHY this approach:**
  - Realistic costs (0.1% commission) prevent overly optimistic backtests
  - Trade-on-close=False simulates realistic execution (no perfect fills)
  - Walk-forward integration ensures temporal validity
  - Financial metrics from Plan 08-01 provide comprehensive assessment
  </action>

  <verify>
  ```bash
  # Syntax check
  python3 -c "from trading.ml.deep_learning.backtesting.runner import BacktestRunner; from trading.ml.deep_learning.backtesting.config import BacktestConfig; print('✓ Imports work')"

  # Test configuration
  python3 -c "
  from trading.ml.deep_learning.backtesting.config import BacktestConfig

  config = BacktestConfig()
  print(f'✓ Config created:')
  print(f'  Commission: {config.commission:.3f}')
  print(f'  Slippage: {config.slippage_bps} bps')
  print(f'  Trade-on-close: {config.trade_on_close}')
  print(f'  Initial cash: \${config.initial_cash:,.0f}')
  "
  ```
  </verify>

  <done>
  - BacktestConfig created with realistic costs (commission 0.001, slippage 15 bps)
  - BacktestRunner created with single and walk-forward backtesting
  - Integration with Plan 08-01 metrics (comprehensive_metrics)
  - Structured logging integrated
  - Slippage modeling documented
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement Performance Report Generator</name>

  <files>
  trading/ml/evaluation/report_generator.py
  requirements.txt
  </files>

  <action>
  Create comprehensive performance report generator:

  **1. Install visualization libraries:**
  - Add `matplotlib>=3.7.0` to requirements.txt
  - Add `seaborn>=0.12.0` to requirements.txt
  - Run `pip install matplotlib seaborn`

  **2. Implement report generator in `trading/ml/evaluation/report_generator.py`:**

  PerformanceReportGenerator class with methods:
  
  a) generate_single_model_report(model_name, backtest_results, save_path):
     - Aggregates metrics across all folds
     - Calculates summary statistics (mean, std)
     - Generates visualizations (equity curves, drawdown, metrics evolution)
     - Saves text report
     - Returns summary dict
  
  b) generate_comparison_report(model_results, save_path):
     - Generates reports for each model
     - Creates comparison DataFrame
     - Plots model comparison visualizations
     - Saves comparison table CSV
     - Returns comparison DataFrame
  
  Visualization methods:
  - _plot_equity_curves(): Equity curves for all folds
  - _plot_drawdown_distribution(): Max drawdown histogram
  - _plot_metrics_over_folds(): Sharpe, Sortino, return, win rate evolution
  - _plot_model_comparison(): Bar charts comparing models
  - _save_text_report(): Text summary report

  **3. Add structured logging for report generation**

  **WHY this approach:**
  - Comprehensive visualizations (equity curves, drawdown, metrics evolution)
  - Statistical aggregation across folds (mean, std)
  - Model comparison framework
  - Both visual and text reports
  </action>

  <verify>
  ```bash
  # Install visualization libraries
  pip install matplotlib seaborn

  # Syntax check
  python3 -c "from trading.ml.evaluation.report_generator import PerformanceReportGenerator; print('✓ Imports work')"

  # Test report generator initialization
  python3 -c "
  from trading.ml.evaluation.report_generator import PerformanceReportGenerator
  from pathlib import Path

  generator = PerformanceReportGenerator(output_dir=Path('reports/test'))
  print('✓ Report generator created')
  print(f'✓ Output directory: {generator.output_dir}')
  "
  ```
  </verify>

  <done>
  - PerformanceReportGenerator class created
  - Single model report generation with visualizations
  - Model comparison report generation
  - 4 visualization types (equity curves, drawdown, metrics evolution, comparison)
  - Text report export
  - matplotlib and seaborn installed
  - Structured logging integrated
  </done>
</task>

<task type="auto">
  <name>Task 4: Create Integration Tests and Example Runner</name>

  <files>
  tests/test_backtesting_integration.py
  examples/run_backtest_comparison.py
  </files>

  <action>
  Create integration tests and example usage:

  **1. Create integration tests in `tests/test_backtesting_integration.py`:**

  TestBacktestingIntegration class with test methods:
  - test_backtest_config_realistic_costs(): Verify config has commission=0.001, slippage=15 bps
  - test_precompute_predictions_no_look_ahead(): Verify predictions are precomputed Series
  - test_backtest_runner_single_backtest(): Test single backtest execution
  - test_report_generator_creates_files(): Test report generator creates output files
  - test_comparison_report_multiple_models(): Test comparison report generation

  **2. Create example runner in `examples/run_backtest_comparison.py`:**

  Complete workflow demonstrating:
  1. Fetch historical data (DataFetcher)
  2. Engineer features (FeatureEngineer from Phase 5)
  3. Load trained models (ModelPersistence from Phase 7)
  4. Prepare data (feature columns, labels)
  5. Run backtests with walk-forward validation
  6. Generate performance reports
  7. Print comparison summary

  **3. Add execution permissions:**
  ```bash
  chmod +x examples/run_backtest_comparison.py
  ```

  **4. Coverage target: 85%+ for backtesting modules**

  **WHY this structure:**
  - Integration tests verify end-to-end workflow
  - Example runner provides production usage pattern
  - Tests confirm realistic cost modeling
  - Validates no look-ahead bias in predictions
  </action>

  <verify>
  ```bash
  # Run integration tests
  pytest tests/test_backtesting_integration.py -v

  # Check coverage
  pytest tests/test_backtesting_integration.py \
    --cov=trading.ml.deep_learning.backtesting \
    --cov=trading.ml.evaluation.report_generator \
    --cov-report=term-missing

  # Verify example runner syntax
  python3 -c "import sys; sys.path.insert(0, 'examples'); from run_backtest_comparison import main; print('✓ Example runner syntax valid')"
  ```
  </verify>

  <done>
  - test_backtesting_integration.py created with 5 test methods
  - Realistic cost modeling test (commission, slippage)
  - Look-ahead bias prevention test
  - Single backtest and comparison report tests
  - Example runner created (run_backtest_comparison.py)
  - Execution permissions set
  - Coverage >85% for backtesting modules
  - All tests pass
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All imports work (no ModuleNotFoundError)
- [ ] DeepLearningStrategy integrates with backtesting.py
- [ ] Predictions are precomputed (no look-ahead bias)
- [ ] Realistic costs configured (commission 0.001, slippage 15 bps)
- [ ] Performance reports generate successfully
- [ ] Model comparison framework works
- [ ] Integration tests pass with >85% coverage
- [ ] Example runner executes without errors
- [ ] No new errors or warnings introduced
</verification>

<success_criteria>
- backtesting.py Strategy wrapper for BiLSTM and Transformer models
- Precomputed predictions pattern (prevents look-ahead bias)
- Realistic cost modeling (commission 0.001, slippage 15 bps, trade-on-close=False)
- Walk-forward backtest runner
- Comprehensive performance reports with visualizations
- Model comparison framework
- Integration tests with >85% coverage
- Example runner demonstrating full workflow
- Ready for production model evaluation
</success_criteria>

<output>
After completion, create `.planning/phases/08-model-evaluation-backtesting/08-02-SUMMARY.md`:

# Phase 8 Plan 2 Summary: Backtesting Integration & Performance Reports

**Integrated deep learning models with backtesting.py framework, implemented realistic trading simulation, and built comprehensive performance reporting infrastructure**

## Accomplishments

- backtesting.py Strategy wrapper with precomputed predictions (prevents look-ahead bias)
- Realistic cost modeling (0.1% commission, 15 bps slippage, realistic execution)
- Walk-forward backtest runner with comprehensive metrics
- Performance report generator with 4 visualization types
- Model comparison framework (BiLSTM vs Transformer vs Ensemble vs Baseline)
- Integration tests (>85% coverage)

## Files Created/Modified

- `trading/ml/deep_learning/backtesting/__init__.py` - Module exports
- `trading/ml/deep_learning/backtesting/strategy.py` - DeepLearningStrategy wrapper
- `trading/ml/deep_learning/backtesting/runner.py` - BacktestRunner class
- `trading/ml/deep_learning/backtesting/config.py` - BacktestConfig with realistic costs
- `trading/ml/evaluation/report_generator.py` - PerformanceReportGenerator class
- `tests/test_backtesting_integration.py` - Integration tests
- `examples/run_backtest_comparison.py` - Example runner
- `requirements.txt` - Added backtesting>=0.3.3, matplotlib>=3.7.0, seaborn>=0.12.0

## Decisions Made

- backtesting.py chosen over vectorbt (simpler, open-source, sufficient for needs)
- Precomputed predictions pattern (prevents look-ahead bias)
- Realistic costs: 0.1% commission (includes slippage estimate), trade-on-close=False
- Walk-forward integration with Plan 08-01 metrics
- 4 visualization types: equity curves, drawdown distribution, metrics evolution, comparison

## Issues Encountered

[None expected - follows backtesting.py documentation patterns]

## Phase 8 Completion

**All Plans Complete:**
- Plan 08-01: Walk-Forward Validation & Metrics ✅
- Plan 08-02: Backtesting Integration & Reports ✅

**Production Readiness:**
- Comprehensive model evaluation infrastructure complete
- Realistic trading simulation with proper cost modeling
- Performance reporting with actionable insights
- Ready for production model selection and deployment

**Next Steps:**
1. Train BiLSTM and Transformer models (Phase 7 training scripts)
2. Run backtest comparison (examples/run_backtest_comparison.py)
3. Select best performing model based on Sharpe ratio and max drawdown
4. Deploy selected model to production
</output>

---
phase: 08-model-evaluation-backtesting
plan: 08-01
type: execute
---

<objective>
Implement walk-forward validation and financial performance metrics for evaluating BiLSTM and Transformer models.

**Purpose:** Establish rigorous time-series evaluation infrastructure that prevents data leakage and provides comprehensive financial performance assessment.

**Output:**
- Walk-forward validation module with chronological splits
- Financial metrics module (Sharpe, Sortino, max drawdown, win rate)
- Comprehensive unit tests
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-model-evaluation-backtesting/08-RESEARCH.md

**Prior decisions affecting this phase:**
- Phase 7: BiLSTM and Transformer models trained with 86 features (from Phase 5)
- Phase 7: Security-hardened model persistence (state_dict only, no unsafe serialization)
- Phase 5: FeatureEngineer produces 86 features (must reuse for consistent evaluation)
- Phase 4: Structured logging and custom exceptions required
- Phase 1: No unsafe serialization (applies to all persistence)

**Research findings (from 08-RESEARCH.md):**
- Walk-forward validation is MANDATORY for time-series (prevents look-ahead bias)
- NEVER use standard k-fold CV (causes massive data leakage)
- Fit scaler ONLY on training data (most common backtest failure mode)
- Use empyrical library for financial metrics (proper annualization, NaN handling)
- Target metrics: Sharpe ratio, Sortino ratio (downside-only), max drawdown, win rate

**Codebase constraints:**
- Reuse Phase 5's FeatureEngineer for 86 features (trading/ml/feature_engineering.py)
- Follow Phase 4 structured logging patterns (trading/logging_config.py)
- Use Phase 4 custom exceptions (trading/exceptions.py)
- Models from Phase 7 in trading/ml/deep_learning/ (BiLSTMClassifier, TransformerClassifier)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Walk-Forward Validation Module</name>

  <files>
  trading/ml/evaluation/__init__.py
  trading/ml/evaluation/walk_forward.py
  </files>

  <action>
  Create walk-forward validation module in `trading/ml/evaluation/walk_forward.py`:

  **1. WalkForwardValidator class with chronological splits:**
  - Parameters: initial_train_size (default 252 for 1 year), test_size (default 60 for 3 months), step_size (default 30 for 1 month advance)
  - Method: `validate(model, X, y)` that performs walk-forward CV
  - Returns: DataFrame with fold results (fold number, train/test indices, accuracy, precision, recall, predictions, actuals)

  **2. CRITICAL: Prevent data leakage from 08-RESEARCH.md Pitfall 1:**
  - Fit scaler ONLY on training data in each fold (NOT on entire dataset)
  - Use chronological splits (NO shuffle anywhere)
  - Verify temporal order: train_end < test_start always

  **3. Integration with Phase 5 FeatureEngineer:**
  - Accept pre-engineered features (86 features from FeatureEngineer)
  - Do NOT re-engineer features inside validator
  - Assume features are already prepared (validator handles splits only)

  **4. Implementation pattern from 08-RESEARCH.md:**
  ```python
  def walk_forward_cv(model, X, y, initial_train_size=252, test_size=60, step_size=30):
      results = []
      train_end = initial_train_size
      fold = 1

      while train_end + test_size <= len(X):
          # Chronological split (NO shuffle!)
          X_train = X.iloc[:train_end]
          y_train = y.iloc[:train_end]
          X_test = X.iloc[train_end:train_end + test_size]
          y_test = y.iloc[train_end:train_end + test_size]

          # Fit scaler ONLY on training data
          scaler = StandardScaler()
          scaler.fit(X_train)
          X_train_scaled = scaler.transform(X_train)
          X_test_scaled = scaler.transform(X_test)  # Use fitted scaler

          # Train on past, predict on future
          model.fit(X_train_scaled, y_train)
          y_pred = model.predict(X_test_scaled)

          # Store results
          results.append({...})

          # Move forward in time
          train_end += step_size
          fold += 1

      return pd.DataFrame(results)
  ```

  **5. Add structured logging:**
  - Use Phase 4's logging_config for progress tracking
  - Log each fold's performance
  - Include DecisionContext for correlation

  **6. Update __init__.py exports**

  **WHY this pattern:**
  - Chronological splits prevent look-ahead bias (08-RESEARCH Pitfall 2)
  - Scaler fitted only on train prevents data leakage (08-RESEARCH Pitfall 1)
  - Walk-forward mimics real trading (train on past, predict future)
  </action>

  <verify>
  ```bash
  # Syntax check
  python3 -c "from trading.ml.evaluation.walk_forward import WalkForwardValidator; print('✓ Imports work')"

  # Quick integration test (no full model training)
  python3 -c "
  from trading.ml.evaluation.walk_forward import WalkForwardValidator
  import pandas as pd
  import numpy as np
  from sklearn.ensemble import RandomForestClassifier

  # Create dummy data
  X = pd.DataFrame(np.random.rand(1000, 86), columns=[f'feature_{i}' for i in range(86)])
  y = pd.Series(np.random.randint(0, 2, 1000))

  # Test walk-forward
  validator = WalkForwardValidator(initial_train_size=252, test_size=60, step_size=30)
  model = RandomForestClassifier(n_estimators=10, random_state=42)
  results = validator.validate(model, X, y)

  print(f'✓ Walk-forward validation works: {len(results)} folds')
  print(f'✓ Average accuracy: {results[\"accuracy\"].mean():.2%}')
  "
  ```
  </verify>

  <done>
  - WalkForwardValidator class created with chronological splits
  - Scaler fitted only on training data (prevents leakage)
  - Integration test passes with dummy data
  - Structured logging integrated
  - Exports added to __init__.py
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Financial Performance Metrics Module</name>

  <files>
  trading/ml/evaluation/metrics.py
  requirements.txt
  </files>

  <action>
  Create financial metrics module in `trading/ml/evaluation/metrics.py`:

  **1. Install empyrical library (from 08-RESEARCH standard stack):**
  - Add `empyrical>=0.5.5` to requirements.txt
  - Run `pip install empyrical`

  **2. Implement comprehensive metrics function:**
  ```python
  def comprehensive_metrics(returns, benchmark_returns=None, risk_free=0.0):
      \"\"\"
      Calculate comprehensive performance metrics.

      Args:
          returns: Series of period returns (not cumulative)
          benchmark_returns: Optional benchmark returns for comparison
          risk_free: Annual risk-free rate (default 0%)

      Returns:
          dict: All performance metrics
      \"\"\"
      # Implementation from 08-RESEARCH.md code examples
  ```

  **3. Metrics to implement (from 08-RESEARCH.md):**
  - **Sharpe ratio**: (annual_return - risk_free) / annual_volatility
  - **Sortino ratio**: (annual_return - risk_free) / downside_volatility (downside-only std)
  - **Max drawdown**: Maximum peak-to-trough decline
  - **Calmar ratio**: annual_return / abs(max_drawdown)
  - **Win rate**: (winning_trades) / total_trades
  - **Total return**: Cumulative return
  - **Annual return**: Annualized return (252 trading days)
  - **Annual volatility**: Annualized standard deviation

  **4. Edge case handling:**
  - NaN/inf values: Replace with 0 or skip metric
  - Zero division: Return 0 for ratios with zero denominator
  - Empty returns series: Return dict with all metrics as 0

  **5. Follow 08-RESEARCH.md formulas exactly:**
  - Sharpe: `np.sqrt(252) * excess_return.mean() / excess_return.std()`
  - Sortino: Use ONLY downside returns for std: `returns[returns < 0].std()`
  - Max drawdown: `(cumulative - running_max) / running_max`, then `.min()`

  **6. Add helper functions:**
  - `calculate_sharpe_ratio(returns, risk_free=0.0, periods=252)`
  - `calculate_sortino_ratio(returns, risk_free=0.0, periods=252)`
  - `calculate_max_drawdown(returns)` - returns dict with max_dd, peak_date, trough_date
  - `calculate_win_rate(trades)` - trades is list/Series of PnLs

  **7. Add structured logging for metric calculation**

  **WHY this approach:**
  - empyrical library is standard for financial metrics (08-RESEARCH)
  - Proper annualization (252 trading days) prevents incorrect risk assessment
  - Sortino ratio uses downside-only volatility (better for asymmetric returns)
  - Max drawdown shows worst-case scenario (critical for risk management)
  </action>

  <verify>
  ```bash
  # Install empyrical
  pip install empyrical

  # Syntax check
  python3 -c "from trading.ml.evaluation.metrics import comprehensive_metrics; print('✓ Imports work')"

  # Test metrics with synthetic data
  python3 -c "
  from trading.ml.evaluation.metrics import comprehensive_metrics
  import pandas as pd
  import numpy as np

  # Create synthetic returns (random walk)
  np.random.seed(42)
  returns = pd.Series(np.random.randn(252) * 0.01 + 0.0005)  # Daily returns

  metrics = comprehensive_metrics(returns, risk_free=0.02)

  print('✓ Metrics calculated:')
  print(f\"  Sharpe ratio: {metrics['sharpe_ratio']:.2f}\")
  print(f\"  Sortino ratio: {metrics['sortino_ratio']:.2f}\")
  print(f\"  Max drawdown: {metrics['max_drawdown']:.2%}\")
  print(f\"  Win rate: {metrics['win_rate']:.2%}\")
  print(f\"  Annual return: {metrics['annual_return']:.2%}\")
  "
  ```
  </verify>

  <done>
  - empyrical library installed
  - comprehensive_metrics() function implemented
  - All 8 financial metrics calculated correctly
  - Helper functions for individual metrics created
  - Edge case handling (NaN, zero division) implemented
  - Verification test passes with synthetic returns
  - Structured logging integrated
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Unit Tests for Walk-Forward Validation and Metrics</name>

  <files>
  tests/test_walk_forward_validation.py
  tests/test_financial_metrics.py
  </files>

  <action>
  Create comprehensive unit tests for evaluation modules:

  **1. Test walk-forward validation (tests/test_walk_forward_validation.py):**

  Test classes:
  - **TestWalkForwardValidator:**
    - `test_chronological_splits()`: Verify train_end < test_start for all folds
    - `test_no_shuffle()`: Verify temporal order maintained (y_train[0] < y_train[-1] < y_test[0])
    - `test_scaler_fitted_on_train_only()`: Mock scaler, verify fit() called only with training data
    - `test_fold_count()`: Verify correct number of folds based on window sizes
    - `test_results_structure()`: Verify returned DataFrame has all required columns

  **2. Test financial metrics (tests/test_financial_metrics.py):**

  Test classes:
  - **TestSharpeRatio:**
    - `test_sharpe_positive_returns()`: Verify > 0 for positive returns
    - `test_sharpe_negative_returns()`: Verify < 0 for negative returns
    - `test_sharpe_zero_volatility()`: Return 0 when std=0 (edge case)

  - **TestSortinoRatio:**
    - `test_sortino_higher_than_sharpe()`: For asymmetric positive returns, Sortino > Sharpe
    - `test_sortino_downside_only()`: Verify uses only returns < 0 for std

  - **TestMaxDrawdown:**
    - `test_max_drawdown_calculation()`: Known drawdown pattern, verify correct
    - `test_peak_trough_dates()`: Verify peak/trough timestamps correct
    - `test_no_drawdown()`: Returns 0 for monotonically increasing returns

  - **TestWinRate:**
    - `test_win_rate_all_wins()`: 100% for all positive trades
    - `test_win_rate_all_losses()`: 0% for all negative trades
    - `test_win_rate_mixed()`: Correct ratio for mixed trades

  **3. Critical test: Prevent data leakage (MOST IMPORTANT from 08-RESEARCH):**
  ```python
  def test_no_data_leakage():
      \"\"\"Verify scaler is fitted ONLY on training data.\"\"\"
      # Create data where test set has different distribution
      # If scaler sees test data, metrics will be suspiciously good
      # This test catches Pitfall 1 from research
  ```

  **4. Coverage target: 90%+ for both modules**

  **5. Use pytest fixtures for reusable test data:**
  - `returns_series()`: Standard return series
  - `drawdown_series()`: Series with known drawdown
  - `ohlcv_data()`: Dummy OHLCV for walk-forward

  **WHY these tests:**
  - Chronological split verification prevents look-ahead bias (08-RESEARCH Pitfall 2)
  - Scaler fitting test prevents data leakage (08-RESEARCH Pitfall 1 - #1 cause of failure)
  - Edge case tests prevent production crashes (NaN, zero division)
  - Win rate test ensures trade PnL tracking correct
  </action>

  <verify>
  ```bash
  # Run tests
  pytest tests/test_walk_forward_validation.py tests/test_financial_metrics.py -v

  # Check coverage
  pytest tests/test_walk_forward_validation.py tests/test_financial_metrics.py \
    --cov=trading.ml.evaluation.walk_forward \
    --cov=trading.ml.evaluation.metrics \
    --cov-report=term-missing

  # Expected output: >90% coverage, all tests pass
  ```
  </verify>

  <done>
  - test_walk_forward_validation.py created with 5 test methods
  - test_financial_metrics.py created with 4 test classes, 11 test methods
  - Critical data leakage test implemented
  - Chronological split verification test passes
  - All edge cases tested (NaN, zero division, empty data)
  - Coverage >90% for both modules
  - All tests pass
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All imports work (no ModuleNotFoundError)
- [ ] WalkForwardValidator performs chronological splits (no shuffle)
- [ ] Scaler fitted only on training data (data leakage prevented)
- [ ] All 8 financial metrics calculate correctly
- [ ] Unit tests pass with >90% coverage
- [ ] No new errors or warnings introduced
</verification>

<success_criteria>
- WalkForwardValidator class created with chronological splits
- Financial metrics module with 8 metrics (Sharpe, Sortino, max drawdown, Calmar, win rate, returns, volatility)
- empyrical library integrated
- Data leakage prevented (scaler fitted only on training data)
- Comprehensive unit tests with >90% coverage
- All tests pass
- Ready for Plan 08-02 (backtesting integration)
</success_criteria>

<output>
After completion, create `.planning/phases/08-model-evaluation-backtesting/08-01-SUMMARY.md`:

# Phase 8 Plan 1 Summary: Walk-Forward Validation & Financial Metrics

**Implemented rigorous time-series evaluation infrastructure with chronological splits and comprehensive financial performance metrics**

## Accomplishments

- Walk-forward validation with chronological splits (prevents look-ahead bias)
- 8 financial performance metrics (Sharpe, Sortino, max drawdown, Calmar, win rate, returns, volatility)
- Data leakage prevention (scaler fitted only on training data)
- Comprehensive unit tests (>90% coverage)

## Files Created/Modified

- `trading/ml/evaluation/__init__.py` - Module exports
- `trading/ml/evaluation/walk_forward.py` - WalkForwardValidator class
- `trading/ml/evaluation/metrics.py` - Financial metrics functions
- `tests/test_walk_forward_validation.py` - Walk-forward unit tests
- `tests/test_financial_metrics.py` - Metrics unit tests
- `requirements.txt` - Added empyrical>=0.5.5

## Decisions Made

- Walk-forward window sizes: 252 days train (1 year), 60 days test (3 months), 30 days step (1 month advance)
- Metrics library: empyrical (standard for financial metrics, proper annualization)
- Scaler strategy: Fit only on training data in each fold (prevents Pitfall 1 from research)
- Risk-free rate: Default 0% (can be configured per metric call)

## Issues Encountered

[None expected - straightforward implementation following research patterns]

## Next Phase Readiness

Ready for Plan 08-02: Backtesting Integration & Reports
- Walk-forward validation ready to evaluate BiLSTM and Transformer models
- Financial metrics ready for backtest performance assessment
- Comprehensive tests ensure evaluation reliability
</output>

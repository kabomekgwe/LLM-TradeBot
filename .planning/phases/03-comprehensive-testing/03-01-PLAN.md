---
phase: 03-comprehensive-testing
plan: 01
type: execute
---

<objective>
Establish test infrastructure with reusable fixtures for market data and mocked exchange.

Purpose: Create foundation for all Phase 3 testing - fixtures reduce test code duplication and enable realistic testing scenarios.
Output: conftest.py with factory fixtures, mock exchange patterns, pytest configuration.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-comprehensive-testing/DISCOVERY.md
@.planning/codebase/STACK.md
@.planning/codebase/TESTING.md

**Codebase constraints:**
- Python 3.7+ with asyncio
- pytest with pytest-asyncio for async tests
- Existing test file: `trading/tests/test_providers.py` (most tests skipped)
- Need 80%+ coverage on agent logic, 100% on risk/state (safety-critical)

**From DISCOVERY.md:**
- pytest-asyncio 1.3.0 patterns (auto mode, async fixtures)
- Factory fixture pattern reduces duplication by 30-60%
- AsyncMock for CCXT exchange mocking
- Parametrized fixtures for scenario testing

**Prior decisions affecting this phase:**
- Phase 2: QuantAnalyst now returns real TA-Lib indicators (fixtures must provide realistic indicator data)
- Phase 2: PredictAgent uses LightGBM (need mock model fixture)
- Phase 1: TradingConfig uses environment variables (fixtures must respect this pattern)

**Why this matters:**
Good fixtures are the foundation of maintainable tests. Factory fixtures reduce duplication, AsyncMock enables testing async exchange operations, and parametrized scenarios ensure comprehensive coverage.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pytest configuration and fixture infrastructure</name>
  <files>pytest.ini, trading/tests/conftest.py, trading/tests/__init__.py</files>
  <action>
**1. Create `pytest.ini`:**
```ini
[pytest]
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function
testpaths = trading/tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    unit: Unit tests (fast, no external dependencies)
    integration: Integration tests (slower, may use mocks)
    slow: Slow tests (model training, large datasets)
    ml: Machine learning model tests
    async: Async tests requiring event loop
    risk: Risk management tests (safety-critical)
```

**2. Create `trading/tests/__init__.py`** (empty file for package recognition)

**3. Create `trading/tests/conftest.py`:**
```python
"""
Shared pytest fixtures for LLM-TradeBot testing.
Provides factory fixtures for market data, mock exchange, and mock models.
"""
import pytest
import pytest_asyncio
from unittest.mock import AsyncMock, Mock
from datetime import datetime
from dataclasses import dataclass
import numpy as np


@dataclass
class OHLCVCandle:
    """OHLCV candle data structure matching trading.types.Candle."""
    timestamp: int
    open: float
    high: float
    low: float
    close: float
    volume: float


@pytest.fixture
def ohlcv_factory():
    """
    Factory fixture for creating OHLCV market data.

    Usage:
        def test_example(ohlcv_factory):
            data = ohlcv_factory(trend="uptrend", num_candles=100)
    """
    def _create_ohlcv(
        symbol: str = "BTC/USDT",
        start_price: float = 30000,
        num_candles: int = 100,
        timeframe: str = "5m",  # '5m', '15m', '1h'
        trend: str = "sideways",  # 'uptrend', 'downtrend', 'sideways'
        volatility: float = 0.02,  # 2% volatility
        volume_range: tuple = (100, 500)
    ):
        """
        Create synthetic OHLCV data for testing.

        Args:
            symbol: Trading pair symbol
            start_price: Starting price for first candle
            num_candles: Number of candles to generate
            timeframe: Candle timeframe (affects timestamp spacing)
            trend: Market trend ('uptrend', 'downtrend', 'sideways')
            volatility: Price volatility as percentage (0.02 = 2%)
            volume_range: Min/max trading volume per candle

        Returns:
            List of OHLCVCandle objects
        """
        import random

        # Timeframe to milliseconds
        timeframe_ms = {
            '5m': 300000,
            '15m': 900000,
            '1h': 3600000
        }.get(timeframe, 300000)

        candles = []
        current_price = start_price
        base_time = int(datetime.now().timestamp() * 1000)

        for i in range(num_candles):
            timestamp = base_time - (num_candles - i) * timeframe_ms

            # Apply trend
            if trend == "uptrend":
                current_price *= 1.001  # 0.1% increase per candle
            elif trend == "downtrend":
                current_price *= 0.999  # 0.1% decrease per candle
            # sideways: no change to current_price

            # Generate OHLC with randomness
            open_price = current_price
            high = open_price * (1 + random.uniform(0, volatility))
            low = open_price * (1 - random.uniform(0, volatility))
            close = random.uniform(low, high)
            volume = random.uniform(*volume_range)

            candles.append(OHLCVCandle(
                timestamp=timestamp,
                open=open_price,
                high=high,
                low=low,
                close=close,
                volume=volume
            ))

            current_price = close

        return candles

    return _create_ohlcv


@pytest_asyncio.fixture
async def mock_exchange():
    """
    Mock CCXT exchange with realistic async API responses.

    Usage:
        async def test_example(mock_exchange):
            data = await mock_exchange.fetch_ohlcv('BTC/USDT', '5m')
    """
    exchange = AsyncMock()

    # Mock fetch_ohlcv (returns list of OHLCV arrays)
    exchange.fetch_ohlcv = AsyncMock(return_value=[
        [1609459200000, 29000, 29500, 28800, 29200, 1250.5],
        [1609462800000, 29200, 29600, 29100, 29400, 1180.2],
    ])

    # Mock fetch_ticker
    exchange.fetch_ticker = AsyncMock(return_value={
        'symbol': 'BTC/USDT',
        'last': 29400,
        'bid': 29398,
        'ask': 29402,
        'baseVolume': 1250.5,
        'quoteVolume': 36750000,
        'timestamp': 1609462800000
    })

    # Mock fetch_balance
    exchange.fetch_balance = AsyncMock(return_value={
        'USDT': {'free': 10000, 'used': 0, 'total': 10000},
        'BTC': {'free': 0.5, 'used': 0, 'total': 0.5}
    })

    # Mock fetch_order_book
    exchange.fetch_order_book = AsyncMock(return_value={
        'bids': [[29398, 1.5], [29397, 2.0]],
        'asks': [[29402, 1.2], [29403, 1.8]]
    })

    # Mock create_order
    exchange.create_order = AsyncMock(return_value={
        'id': 'test-order-123',
        'status': 'closed',
        'symbol': 'BTC/USDT',
        'type': 'limit',
        'side': 'buy',
        'price': 29400,
        'amount': 0.01,
        'filled': 0.01,
        'remaining': 0.0,
        'timestamp': 1609462800000
    })

    # Mock async context manager (for async with exchange: usage)
    exchange.__aenter__ = AsyncMock(return_value=exchange)
    exchange.__aexit__ = AsyncMock(return_value=None)

    return exchange


@pytest.fixture
def mock_lightgbm_model():
    """
    Mock LightGBM Booster for testing agents without training.

    Usage:
        def test_predict_agent(mock_lightgbm_model):
            prediction = mock_lightgbm_model.predict([[65, 0.5, 0.3, 0.2, 30100, 30000, 29900, 0.01]])
    """
    mock_model = Mock()

    # Mock predict (returns numpy array of probabilities)
    mock_model.predict = Mock(return_value=np.array([0.75]))  # 75% probability of price up

    # Mock best_iteration (for early stopping)
    mock_model.best_iteration = 100

    # Mock num_trees
    mock_model.num_trees = Mock(return_value=100)

    return mock_model


# Parametrized market scenarios for comprehensive testing
@pytest.fixture(params=[
    {"name": "strong_uptrend", "trend": "uptrend", "num_candles": 50, "expected_bullish": True},
    {"name": "strong_downtrend", "trend": "downtrend", "num_candles": 50, "expected_bullish": False},
    {"name": "sideways_market", "trend": "sideways", "num_candles": 100, "expected_bullish": None},
])
def market_scenario(request, ohlcv_factory):
    """
    Parametrized fixture providing different market scenarios.

    Usage:
        def test_agent_scenarios(market_scenario):
            data = market_scenario['data']
            expected = market_scenario['expected_bullish']
    """
    params = request.param
    return {
        'name': params['name'],
        'data': ohlcv_factory(
            trend=params['trend'],
            num_candles=params['num_candles']
        ),
        'expected_bullish': params['expected_bullish']
    }
```

**What to avoid:**
- Don't import actual CCXT (use AsyncMock for all exchange operations)
- Don't generate real TA-Lib indicators in fixtures (too slow, defeats mocking purpose)
- Don't hardcode specific test scenarios in conftest (use parametrize in test files)
  </action>
  <verify>
1. Files created: `pytest.ini`, `trading/tests/conftest.py`, `trading/tests/__init__.py`
2. Import test: `python -c "from trading.tests.conftest import ohlcv_factory, mock_exchange; print('OK')"`
3. Pytest discovery: `pytest --collect-only trading/tests/` shows conftest fixtures available
4. No import errors: All fixtures import without ModuleNotFoundError
  </verify>
  <done>
- pytest.ini configured with async mode and markers
- conftest.py created with factory fixtures (ohlcv_factory, mock_exchange, mock_lightgbm_model)
- Parametrized market_scenario fixture for comprehensive testing
- AsyncMock patterns for CCXT exchange operations
- All fixtures importable and discoverable by pytest
  </done>
</task>

<task type="auto">
  <name>Task 2: Create state persistence tests with 100% coverage</name>
  <files>trading/tests/test_state.py</files>
  <action>
Create comprehensive tests for `trading/state.py` covering save/load, crash recovery, atomic writes, corruption handling.

**Why 100% coverage**: State persistence is safety-critical - data loss unacceptable in production.

**Create `trading/tests/test_state.py`:**
```python
"""
State persistence tests (100% coverage required - safety-critical).
Tests atomic writes, crash recovery, corruption handling.
"""
import pytest
import tempfile
import os
from pathlib import Path
from trading.state import TradingState  # Adjust import based on actual module


class TestStatePersistence:
    """Test state save/load operations."""

    def test_save_creates_file(self, tmp_path):
        """Test saving state creates file at specified path."""
        state_file = tmp_path / "state.json"

        state = TradingState()
        state.positions = {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}}
        state.save(state_file)

        assert state_file.exists()
        assert state_file.stat().st_size > 0

    def test_load_restores_state(self, tmp_path):
        """Test loading state restores all fields correctly."""
        state_file = tmp_path / "state.json"

        # Save state
        original_state = TradingState()
        original_state.positions = {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}}
        original_state.last_decision_time = 1609462800
        original_state.save(state_file)

        # Load state
        loaded_state = TradingState.load(state_file)

        assert loaded_state.positions == original_state.positions
        assert loaded_state.last_decision_time == original_state.last_decision_time

    def test_atomic_write_prevents_corruption(self, tmp_path):
        """Test crash during write doesn't corrupt existing state."""
        state_file = tmp_path / "state.json"

        # Save initial state
        state = TradingState()
        state.positions = {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}}
        state.save(state_file)

        # Simulate crash during write by writing partial data
        with open(state_file, 'w') as f:
            f.write('{"positions": {')  # Incomplete JSON

        # Should still be able to load (atomic writes use temp file + rename)
        # OR should raise clear error if corruption detected
        # Adjust assertion based on actual implementation
        try:
            loaded_state = TradingState.load(state_file)
            # If atomic writes work, should load last good state
            assert loaded_state.positions == {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}}
        except Exception as e:
            # If corruption detected, should raise clear error
            assert "corrupt" in str(e).lower() or "invalid" in str(e).lower()

    def test_missing_file_returns_empty_state(self, tmp_path):
        """Test loading non-existent file returns default state."""
        state_file = tmp_path / "nonexistent.json"

        state = TradingState.load(state_file)

        assert state.positions == {}
        assert state.last_decision_time is None or state.last_decision_time == 0

    def test_concurrent_writes_dont_corrupt(self, tmp_path):
        """Test multiple writes in quick succession maintain data integrity."""
        state_file = tmp_path / "state.json"

        # Write 10 times rapidly
        for i in range(10):
            state = TradingState()
            state.positions = {'BTC/USDT': {'size': 0.01 * i, 'entry_price': 30000 + i}}
            state.save(state_file)

        # Load final state
        final_state = TradingState.load(state_file)

        # Should have last written values
        assert final_state.positions['BTC/USDT']['size'] == 0.09
        assert final_state.positions['BTC/USDT']['entry_price'] == 30009


class TestStateValidation:
    """Test state validation and schema checking."""

    def test_invalid_json_raises_error(self, tmp_path):
        """Test loading file with invalid JSON raises clear error."""
        state_file = tmp_path / "invalid.json"
        state_file.write_text("not valid json {{{")

        with pytest.raises(Exception) as exc_info:
            TradingState.load(state_file)

        assert "json" in str(exc_info.value).lower() or "invalid" in str(exc_info.value).lower()

    def test_missing_required_fields_handled(self, tmp_path):
        """Test loading state with missing fields uses defaults."""
        state_file = tmp_path / "partial.json"
        state_file.write_text('{}')  # Empty JSON object

        state = TradingState.load(state_file)

        # Should have default values
        assert hasattr(state, 'positions')
        assert isinstance(state.positions, dict)


class TestStateBackwardCompatibility:
    """Test state loading works with old schemas (future-proofing)."""

    def test_load_old_schema_without_new_fields(self, tmp_path):
        """Test loading old state schema doesn't break when new fields added."""
        state_file = tmp_path / "old_state.json"

        # Write old schema (missing hypothetical future fields)
        import json
        old_schema = {
            'positions': {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}},
            # Missing future fields like 'risk_metrics', 'decision_history', etc.
        }
        state_file.write_text(json.dumps(old_schema))

        # Should load successfully with defaults for missing fields
        state = TradingState.load(state_file)

        assert state.positions == {'BTC/USDT': {'size': 0.01, 'entry_price': 30000}}
        # New fields should have defaults
        # assert state.risk_metrics == {}  # Example of future field


# Run with: pytest trading/tests/test_state.py -v --cov=trading.state --cov-report=term-missing
```

**What to avoid:**
- Don't skip edge cases (corruption, concurrent writes, invalid JSON)
- Don't test implementation details (test behavior, not internal methods)
- Don't hardcode file paths (use pytest tmp_path fixture)
- Don't assume atomic writes work (test them explicitly)

**Why these choices:**
- 100% coverage ensures no data loss scenarios missed
- tmp_path fixture provides isolated test environment
- Testing corruption scenarios validates Phase 1's atomic write implementation
- Backward compatibility tests prepare for future schema evolution
  </action>
  <verify>
1. Test file created: `trading/tests/test_state.py` exists
2. Tests run: `pytest trading/tests/test_state.py -v` shows all tests passing
3. Coverage check: `pytest trading/tests/test_state.py --cov=trading.state --cov-report=term-missing` shows 100% coverage
4. No test failures: All state persistence scenarios pass
  </verify>
  <done>
- State persistence tests with 100% coverage requirement
- Atomic write validation (crash safety from Phase 1)
- Corruption handling tests
- Concurrent write tests
- Invalid JSON handling tests
- Backward compatibility tests for future schema changes
- All tests use tmp_path for isolation
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest.ini configured with asyncio_mode = auto
- [ ] conftest.py created with ohlcv_factory, mock_exchange, mock_lightgbm_model fixtures
- [ ] Parametrized market_scenario fixture for comprehensive scenarios
- [ ] test_state.py created with 100% coverage tests
- [ ] `pytest --collect-only` shows fixtures discoverable
- [ ] `pytest trading/tests/test_state.py -v` passes all tests
- [ ] No import errors when loading fixtures
</verification>

<success_criteria>

- Both tasks completed successfully
- Test infrastructure established (pytest.ini, conftest.py)
- Factory fixtures reduce duplication (ohlcv_factory allows unlimited variations)
- AsyncMock patterns for CCXT exchange mocking
- State persistence tests achieve 100% coverage
- Ready for Plan 03-02 (Agent & Risk Tests)
</success_criteria>

<output>
After completion, create `.planning/phases/03-comprehensive-testing/03-01-SUMMARY.md`:

# Phase 3 Plan 1: Test Infrastructure & Fixtures Summary

**[One-liner describing what shipped]**

## Accomplishments

- Established pytest configuration with async mode and test markers
- Created factory fixtures for OHLCV market data generation
- Implemented AsyncMock patterns for CCXT exchange mocking
- Built mock LightGBM model fixture for agent testing
- Achieved 100% test coverage on state persistence (safety-critical)

## Files Created/Modified

- `pytest.ini` - pytest configuration with async mode
- `trading/tests/conftest.py` - Shared fixtures (ohlcv_factory, mock_exchange, mock_lightgbm_model)
- `trading/tests/__init__.py` - Package marker
- `trading/tests/test_state.py` - State persistence tests (100% coverage)

## Decisions Made

- Factory fixture pattern for OHLCV (reduces duplication 30-60%)
- pytest-asyncio auto mode for async tests
- AsyncMock for exchange operations (no real API calls)
- tmp_path for test isolation
- 100% coverage requirement for state (safety-critical)

## Deviations from Plan

[Document any deviations using deviation rules, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Verification Results

[Results of verification checklist]

## Next Step

Ready for 03-02-PLAN.md (Agent & Risk Tests)
</output>

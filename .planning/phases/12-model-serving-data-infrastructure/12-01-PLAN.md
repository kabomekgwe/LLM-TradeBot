# Phase 12 Plan 01: Database Infrastructure & Model Serving

**Goal**: Implement PostgreSQL + TimescaleDB for trade history persistence and FastAPI endpoints for production model serving

**Context References**:
- @12-CONTEXT.md - User vision for Phase 12
- @STATE.md - Project state and prior decisions
- @ROADMAP.md - Phase 12 goals and dependencies

**Scope**: 4 tasks, ~1,200 LOC, 50% context usage

**Prerequisites**:
- Phase 11 Docker infrastructure complete
- Existing model classes (XGBoost, LightGBM, LSTM) with save/load methods
- File-based `TradeRecord` system in `trading/memory/trade_history.py`

---

## Task 1: PostgreSQL + TimescaleDB Docker Setup

**Type**: `implement`

**Files to create/modify**:
- `docker-compose.yml` (modify - add PostgreSQL service)
- `.env.production.template` (modify - add DB connection vars)
- `requirements.txt` (modify - add `psycopg2-binary`, `sqlalchemy`)

**Action**:
Add TimescaleDB service to Docker Compose using official `timescale/timescaledb:latest-pg17` image.

Configure:
- Volume persistence for database data (`./db_data:/var/lib/postgresql/data`)
- Health check using `pg_isready` command
- Environment variables: `POSTGRES_DB=tradingbot`, `POSTGRES_USER`, `POSTGRES_PASSWORD`
- Resource limits: `TS_TUNE_MEMORY` and `TS_TUNE_NUM_CPUS` for auto-tuning
- Port mapping: `5432:5432` (local dev only, not exposed in production)

Update `.env.production.template` with database connection string:
```
DATABASE_URL=postgresql://tradingbot:${DB_PASSWORD}@postgres:5432/tradingbot
```

Add dependencies:
```
psycopg2-binary>=2.9.9  # PostgreSQL adapter
sqlalchemy>=2.0.23  # Database ORM
alembic>=1.13.1  # Database migrations
```

**Verify**:
```bash
# Start services
docker-compose up -d postgres

# Verify PostgreSQL is healthy
docker-compose ps postgres | grep "healthy"

# Verify TimescaleDB extension
docker exec -it llm-tradebot-postgres-1 psql -U tradingbot -d tradingbot -c "CREATE EXTENSION IF NOT EXISTS timescaledb;"
docker exec -it llm-tradebot-postgres-1 psql -U tradingbot -d tradingbot -c "SELECT extname, extversion FROM pg_extension WHERE extname = 'timescaledb';"
```

**Done**: PostgreSQL + TimescaleDB service running with health checks and auto-tuning

---

## Task 2: Trade History Database Schema

**Type**: `implement`

**Files to create**:
- `trading/database/__init__.py` - Database module initialization
- `trading/database/models.py` - SQLAlchemy models for trade history
- `trading/database/connection.py` - Database connection and session management
- `migrations/versions/001_create_trade_history.py` - Alembic migration for initial schema

**Action**:
Create SQLAlchemy model for trade history matching existing `TradeRecord` dataclass:

```python
# trading/database/models.py
from sqlalchemy import Column, String, Float, Integer, Boolean, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class TradeHistory(Base):
    __tablename__ = 'trade_history'

    # Primary key and indexing
    id = Column(Integer, primary_key=True, autoincrement=True)
    trade_id = Column(String(50), unique=True, nullable=False, index=True)
    symbol = Column(String(20), nullable=False, index=True)
    timestamp = Column(DateTime, nullable=False, index=True)  # Time-series index

    # Trade details
    side = Column(String(10), nullable=False)  # buy/sell
    order_type = Column(String(10), nullable=False)  # market/limit
    amount = Column(Float, nullable=False)
    entry_price = Column(Float, nullable=False)
    exit_price = Column(Float, nullable=True)

    # Performance
    realized_pnl = Column(Float, default=0.0)
    pnl_pct = Column(Float, default=0.0)
    fees = Column(Float, default=0.0)

    # Context
    market_regime = Column(String(20), nullable=True)
    bull_confidence = Column(Float, nullable=True)
    bear_confidence = Column(Float, nullable=True)
    decision_confidence = Column(Float, nullable=True)

    # Outcome
    won = Column(Boolean, default=False)
    closed = Column(Boolean, default=False)
    close_timestamp = Column(DateTime, nullable=True)

    # Agent insights (JSON for flexibility)
    agent_votes = Column(JSON, nullable=True)
    signals = Column(JSON, nullable=True)
```

Convert to TimescaleDB hypertable for time-series optimization:
```sql
-- migrations/versions/001_create_trade_history.py
SELECT create_hypertable('trade_history', 'timestamp', if_not_exists => TRUE);
```

Create database connection manager:
```python
# trading/database/connection.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import os

DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://tradingbot:password@localhost:5432/tradingbot')

engine = create_engine(DATABASE_URL, pool_pre_ping=True, pool_size=5, max_overflow=10)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    """Get database session."""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

**Verify**:
```bash
# Run migration
docker-compose run --rm trading-bot alembic upgrade head

# Verify hypertable created
docker exec -it llm-tradebot-postgres-1 psql -U tradingbot -d tradingbot -c "SELECT * FROM timescaledb_information.hypertables WHERE hypertable_name = 'trade_history';"

# Verify indexes
docker exec -it llm-tradebot-postgres-1 psql -U tradingbot -d tradingbot -c "\d trade_history"
```

**Done**: Trade history schema created as TimescaleDB hypertable with proper indexes

---

## Task 3: Migrate Trade History to Database

**Type**: `implement`

**Files to create/modify**:
- `trading/database/repositories/trade_repository.py` - Repository pattern for trade CRUD operations
- `trading/memory/trade_history.py` (modify - add database fallback)
- `trading/manager.py` (modify - initialize database connection)

**Action**:
Create repository for trade operations:

```python
# trading/database/repositories/trade_repository.py
from typing import List, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from ..models import TradeHistory

class TradeRepository:
    """Repository for trade history operations."""

    def __init__(self, db: Session):
        self.db = db

    def create_trade(self, trade_record: dict) -> TradeHistory:
        """Insert new trade record."""
        trade = TradeHistory(**trade_record)
        self.db.add(trade)
        self.db.commit()
        self.db.refresh(trade)
        return trade

    def update_trade(self, trade_id: str, updates: dict) -> Optional[TradeHistory]:
        """Update existing trade (e.g., close position)."""
        trade = self.db.query(TradeHistory).filter_by(trade_id=trade_id).first()
        if trade:
            for key, value in updates.items():
                setattr(trade, key, value)
            self.db.commit()
            self.db.refresh(trade)
        return trade

    def get_recent_trades(self, limit: int = 100) -> List[TradeHistory]:
        """Get recent trades ordered by timestamp."""
        return self.db.query(TradeHistory).order_by(TradeHistory.timestamp.desc()).limit(limit).all()

    def get_trades_by_symbol(self, symbol: str, start_date: Optional[datetime] = None) -> List[TradeHistory]:
        """Get trades for specific symbol."""
        query = self.db.query(TradeHistory).filter_by(symbol=symbol)
        if start_date:
            query = query.filter(TradeHistory.timestamp >= start_date)
        return query.order_by(TradeHistory.timestamp.desc()).all()
```

Modify `TradeJournal` to use database:

```python
# trading/memory/trade_history.py (modify __init__ and methods)
from ..database.connection import get_db
from ..database.repositories.trade_repository import TradeRepository

class TradeJournal:
    def __init__(self, data_dir: Optional[Path] = None, use_database: bool = True):
        self.use_database = use_database
        self.data_dir = data_dir or Path("data/trades")

        if self.use_database:
            self.db = next(get_db())
            self.repo = TradeRepository(self.db)
        else:
            # File-based fallback
            self.data_dir.mkdir(parents=True, exist_ok=True)

    def record_trade(self, trade: TradeRecord):
        """Record trade to database (or file as fallback)."""
        if self.use_database:
            self.repo.create_trade(trade.to_dict())
        else:
            # Existing file-based implementation
            self._save_to_file(trade)
```

**Verify**:
```bash
# Run trading bot in test mode
docker-compose run --rm trading-bot python -m trading.cli run --testnet

# Execute a test trade and verify database insertion
docker exec -it llm-tradebot-postgres-1 psql -U tradingbot -d tradingbot -c "SELECT trade_id, symbol, timestamp, side, realized_pnl FROM trade_history ORDER BY timestamp DESC LIMIT 5;"

# Verify file-based fallback (stop postgres, run bot, check data/trades/ files created)
docker-compose stop postgres
docker-compose run --rm trading-bot python -m trading.cli run --testnet --dry-run
ls data/trades/
```

**Done**: Trade history persists to PostgreSQL with file-based fallback

---

## Task 4: FastAPI Model Serving Endpoints

**Type**: `implement`

**Files to create**:
- `trading/api/ml_serving.py` - Model serving endpoints
- `trading/ml/model_loader.py` - Singleton model loader with caching

**Files to modify**:
- `trading/web/server.py` - Register ML serving routes

**Action**:
Create model loader with caching:

```python
# trading/ml/model_loader.py
from typing import Dict, Optional
from pathlib import Path
import logging
from .models.xgboost_model import XGBoostModel
from .models.lightgbm_model import LightGBMModel
from .models.lstm_model import LSTMModel

class ModelLoader:
    """Singleton model loader with in-memory caching."""

    _instance = None
    _models: Dict[str, any] = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.model_dir = Path("models")

    def load_model(self, model_name: str, model_type: str) -> Optional[any]:
        """Load model from disk or return cached instance."""
        cache_key = f"{model_type}_{model_name}"

        if cache_key in self._models:
            self.logger.info(f"Returning cached model: {cache_key}")
            return self._models[cache_key]

        model_path = self.model_dir / model_name
        if not model_path.exists():
            self.logger.error(f"Model file not found: {model_path}")
            return None

        try:
            if model_type == "xgboost":
                model = XGBoostModel.load_model(str(model_path))
            elif model_type == "lightgbm":
                model = LightGBMModel.load_model(str(model_path))
            elif model_type == "lstm":
                model = LSTMModel.load_model(str(model_path))
            else:
                raise ValueError(f"Unsupported model type: {model_type}")

            self._models[cache_key] = model
            self.logger.info(f"Loaded and cached model: {cache_key}")
            return model
        except Exception as e:
            self.logger.error(f"Failed to load model {cache_key}: {e}")
            return None
```

Create FastAPI serving endpoints:

```python
# trading/api/ml_serving.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
import numpy as np
from ..ml.model_loader import ModelLoader

router = APIRouter(prefix="/api/v1/ml", tags=["ml-serving"])

class PredictionRequest(BaseModel):
    model_name: str
    model_type: str  # xgboost, lightgbm, lstm
    features: List[List[float]]  # 2D array of features

class PredictionResponse(BaseModel):
    predictions: List[float]
    model_name: str
    model_type: str

@router.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Serve predictions from cached models."""
    loader = ModelLoader()
    model = loader.load_model(request.model_name, request.model_type)

    if model is None:
        raise HTTPException(status_code=404, detail=f"Model {request.model_name} not found")

    try:
        X = np.array(request.features)
        predictions = model.predict(X)

        return PredictionResponse(
            predictions=predictions.tolist(),
            model_name=request.model_name,
            model_type=request.model_type
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@router.get("/models")
async def list_models():
    """List available models."""
    from pathlib import Path
    model_dir = Path("models")

    if not model_dir.exists():
        return {"models": []}

    models = [
        {"name": f.name, "size_mb": round(f.stat().st_size / (1024 * 1024), 2)}
        for f in model_dir.iterdir()
        if f.is_file() and f.suffix in ['.pkl', '.h5', '.pth']
    ]

    return {"models": models}
```

Register routes in `trading/web/server.py`:

```python
from ..api.ml_serving import router as ml_router

app.include_router(ml_router)
```

**Verify**:
```bash
# Start services
docker-compose up -d

# Test model listing
curl http://localhost:5173/api/v1/ml/models

# Test prediction (ensure models exist in ./models/)
curl -X POST http://localhost:5173/api/v1/ml/predict \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "xgboost_model.pkl",
    "model_type": "xgboost",
    "features": [[1.5, 2.3, 0.8, 1.2, 0.5]]
  }'

# Verify model caching (check logs for "Returning cached model")
docker-compose logs trading-bot | grep "cached model"
```

**Done**: FastAPI endpoints serving ML predictions with model caching

---

## Success Criteria

- [ ] PostgreSQL + TimescaleDB service running in Docker Compose with health checks
- [ ] Trade history schema created as hypertable with proper indexes
- [ ] Trades persist to database with file-based fallback working
- [ ] FastAPI `/api/v1/ml/predict` endpoint serves predictions from cached models
- [ ] `/api/v1/ml/models` endpoint lists available models
- [ ] Database migrations run successfully via Alembic
- [ ] All manual verification commands pass

## Checkpoints

- **After Task 2** (human-verify): Verify database schema and hypertable creation
- **After Task 4** (human-verify): Test model serving endpoint with real model

---

**Sources**:
- [TimescaleDB Docker Setup Guide](https://compositecode.blog/2025/03/28/setup-timescaledb-with-docker-compose-a-step-by-step-guide/)
- [TimescaleDB Official Docker Image](https://hub.docker.com/r/timescale/timescaledb)
- [FastAPI PyTorch Model Serving](https://medium.com/@mingc.me/deploying-pytorch-model-to-production-with-fastapi-in-cuda-supported-docker-c161cca68bb8)
- [FastAPI Model Serving Production Patterns](https://markaicode.com/fastapi-uvicorn-model-serving-production/)

---

*Phase: 12-model-serving-data-infrastructure*
*Plan: 12-01*
*Created: 2025-12-28*
